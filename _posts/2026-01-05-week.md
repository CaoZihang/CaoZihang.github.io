---
layout: post
title:  "【阅】本周阅读摘选2025-12-29 → 2026-01-04"
author: "Cao Zihang"
header-style: text
catalog: true
status: "working"
mathjax: true
tags:
  - 日常
  
  
---
<center style="margin-bottom: 20px; margin-top: 50px"><font color="#3879B1" style="line-height: 1.4;font-weight: 700;font-size: 36px;box-sizing: border-box; ">本周阅读摘选</font></center>


<center style=" margin-bottom: 30px;">2025-12-29 → 2026-01-04</center>

<font style="font-weight: bold;">目录</font>

* 目录
{:toc}


# 学术相关

## Designing information to engage customers [^1]

## [集智俱乐部丨综述：信息论如何成为复杂系统科学的核心工具](https://mp.weixin.qq.com/s/WLId6Cs9sd77NN_8LN1urQ)[^2]

复杂系统：具有高度互联性、非线性、多尺度和动态性等特征，常表现出临界相变、级联故障和自组织等现象

- 香农熵

$H(X) = - \sum_{x \in \mathcal{X} p(x) \log (p(x))$

对数底通常为2，单位为比特

- 联合熵H(X,Y)与条件熵H(Y\|X)

不妨令$H(Y) \leq H(X)$，则$H(Y)\leq H(X) \leq H(X,Y) \leq H(X) + H(Y)$

$H(X,Y) = H(X) + H(Y\|X)$

$\therefore$ 当且仅当$Y$完全由$X$决定$H(Y\|X)=0$时，联合熵等于最大的单熵；当且仅当两个变量完全独立$H(Y\|X) \geq H(Y)$时，联合熵等于两个单熵之和

- 相对熵 Kullback-Leibler散度

$D_{KL}(P\|\|Q)=\sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}$

假设$Q(X)$为先验概率分布，$P(X)$为后验概率分布，则KL散度为将先验信念修正为P时获取的信息

KL散度可以被视作描述两个分布的“距离”，但不能假设$D_{KL}(P\|\|Q) = D_{KL}(Q\|\|P)$或假设满足三角不等式

修正的KL散度Jensen-Shannon散度满足对称性

$\begin{align}D_{JS}(P\|\|Q) = \frac{D_{KL}(P\|\|M)+ D_{KL}(Q\|\|M)}{2}\\ M := \frac{P+Q}{2}\end{align}$

$D_{JS}$不是距离指标，$\sqrt{D_{JS}(P\|\|Q)}$为$P$到$Q$之间的Jensen-Shannon距离

- 局部相对熵

即将相对熵的期望“展开”

$d_{kL}(P(x)\|\|Q(x)) = log \frac{P(x)}{Q(x)} = h_Q(x) - h_P(x)$

- 互信息I(X;Y)

互信息量化了两个变量的（任何形式的）统计依赖关系

$I(X;Y)=H(X) +H(Y) - H(X,Y)=D_{KL}(P(X,Y)\|\|P(X)P(Y))=\sum_{\begin{align}X\in\mathcal{X}_1\\Y\in\mathcal{Y}  \end{align}}P(X,Y)log\frac{P(X\|Y)}{P(X)}$

- 局部互信息

$i(X,Y)=h(X) - h(X\|Y)=log \frac{P(X,Y)}{P(X)P(Y)}$

- 条件互信息

$I(X_1; X_2\| X_3) = H(X_1,X_3) + H(X_2,X_3) - H(X_1,X_2,X_3) - H(X_3)$

### 部分信息分解 partial information decomposition (PID)

对于$I(S;X,Y)$由X和Y决定的关于目标S的总信息

- 冗余信息 (Redundancy): 由X和Y各自单独提供的，关于S的相同信息
- 特有信息 (Unique): 仅由X或Y提供的，关于S的独特信息
- 协同信息 (Synergy): 仅当同时考虑X和Y时，产生关于S的信息

$I(S;X,Y) = Red(X,Y\to S) + X\ Unq(X\to S\|Y) + Y\ Unq(Y\to S\|X) + Syn(X,Y\to S)$

当源数$N>2$时PID迅速复杂化，需要使用Redundancy Lattice等方法分解

### 传递熵 transfer entropy

传递熵为互信息在时间序列熵的推广，衡量在已知$Y$自身过去历史的情况下，$X$过去历史能为预测$Y$当前状态提供多少额外信息，从而推断出因果关系的方向。

局限性：能够检验统计依赖性，但无法完全确定因果关系（不能排除存在未观测的共同驱动变量）；对弱平稳过程

$T_{Y\to X} = H(X_{t+1}\|X_t^{(k)}) - H(X_{t+1}\|X_t^{(k)},Y_t^{(l)})= I(Y_{t}^{(l);X_{t+1}\|X_t^{(k)})$

[^1]: Guo, L. (2025). Designing information to engage customers. Management Science, 71(10), 8169–8187. https://doi.org/10.1287/mnsc.2022.03503
